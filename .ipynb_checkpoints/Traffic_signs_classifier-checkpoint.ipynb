{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "folder_name=os.listdir('train')\n",
    "labels=[]\n",
    "images=[]\n",
    "cont=0\n",
    "for folder in folder_name:\n",
    "    for image in os.listdir('train/'+folder):\n",
    "        img=mpimg.imread('train/'+folder+'/'+image)\n",
    "        resized = cv2.resize(img, (32,32), interpolation = cv2.INTER_AREA) \n",
    "        images.append(resized)\n",
    "        labels.append(int(folder[-2:]))\n",
    "    \n",
    "print('images: ',len(images))\n",
    "print('labels: ',len(labels))\n",
    "print('number of classes ',len(set(labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images:  11770\n"
     ]
    }
   ],
   "source": [
    "folder_name=os.listdir('test_files')\n",
    "test_images=[]\n",
    "cont=0\n",
    "for image in os.listdir('test_files'):\n",
    "    img=mpimg.imread('test_files/'+image)\n",
    "    resized = cv2.resize(img, (32,32), interpolation = cv2.INTER_AREA) \n",
    "    test_images.append(resized)\n",
    "print('images: ',len(test_images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir('test_files'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the size of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(np.shape(images[-i]))\n",
    "print(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the last image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[-1])\n",
    "plt.show()\n",
    "print(np.max(images[-1]),np.min(images[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a function to agment the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Data exploration visualization code goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "colors = ['blue']\n",
    "labels_ = ['train', 'valid', 'test']\n",
    "counts,ranges,_=plt.hist([labels],list(range(n_classes+1)), stacked=True , color=colors, label=labels)#n_classes-1)\n",
    "plt.xlabel('classes')\n",
    "plt.ylabel('# samples')\n",
    "_=plt.legend()\n",
    "#plt.savefig('./examples/data_distribution.png')\n",
    "number_train  = np.histogram(labels,list(range(n_classes+1)))[0]\n",
    "#number_valid  = np.histogram(y_valid,list(range(n_classes+1)))[0]\n",
    "#number_test =   np.histogram(y_test,list(range(n_classes+1)))[0]\n",
    "#print('counts',counts)\n",
    "print('median value of train samples',np.median(number_train))\n",
    "#print('median value of validation samples',np.median(number_valid))\n",
    "#print('median value of test samples',np.median(number_test))\n",
    "#print(np.histogram(y_test,list(range(n_classes+1)))[0])\n",
    "number_training_samples = dict(zip(range(n_classes), number_train))\n",
    "print('number of training samples',number_training_samples)\n",
    "#number_validation_samples = dict(zip(range(n_classes), number_valid))\n",
    "#print('number of validation samples',number_validation_samples)\n",
    "#number_test_samples = dict(zip(range(n_classes), number_test))\n",
    "#print('number of test samples',number_test_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'number_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-00c4df5ace24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0maugmentation_required\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0maugmentation_required_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mAugmentation_required\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'class id : # new samples required'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maugmentation_required_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'number_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def Augmentation_required(histogram,percentage_under=0.7):\n",
    "    ## this function receives a histogram of a dataset as a list and returns a dictionary with the ##\n",
    "    #labels which need augmentation as keys and the minimum number of images required\n",
    "    augmentation_required={}\n",
    "    mean_value=np.median(histogram)\n",
    "    bad_indices=np.nonzero(histogram<percentage_under*mean_value)[0]\n",
    "    for index in bad_indices:\n",
    "        augmentation_required[index]=int(mean_value-histogram[index])\n",
    "    return augmentation_required\n",
    "\n",
    "augmentation_required_train=Augmentation_required(number_train)\n",
    "print('class id : # new samples required')\n",
    "print(augmentation_required_train)\n",
    "print(sum(augmentation_required_train.values()), 'images from data augmentation are required in the train set')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\"\"\"\n",
    "def augment_brightness_camera_images(image):\n",
    "    image1 = cv2.cvtColor(image,cv2.COLOR_RGB2HSV)\n",
    "    image1 = np.array(image1, dtype = np.float64)\n",
    "    random_bright = .5+np.random.uniform()\n",
    "    image1[:,:,2] = image1[:,:,2]*random_bright\n",
    "    image1[:,:,2][image1[:,:,2]>255]  = 255\n",
    "    image1 = np.array(image1, dtype = np.uint8)\n",
    "    image1 = cv2.cvtColor(image1,cv2.COLOR_HSV2RGB)\n",
    "    return image1\n",
    "\"\"\"\n",
    "\n",
    "def brightness_augment(img, factor=0.5): \n",
    "    ## This function modifies the brightness of an image to be +-factor higher or lower\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV) #convert to hsv\n",
    "    hsv = np.array(hsv, dtype=np.float64)\n",
    "    hsv[:, :, 2] = hsv[:, :, 2] * (factor + np.random.uniform()) #scale channel V uniformly\n",
    "    hsv[:, :, 2][hsv[:, :, 2] > 255] = 255 #reset out of range values\n",
    "    rgb = cv2.cvtColor(np.array(hsv, dtype=np.uint8), cv2.COLOR_HSV2RGB)\n",
    "    return rgb\n",
    "\n",
    "def add_salt_pepper_noise(X_img):\n",
    "    #this function adds random white and black points to an image\n",
    "    X_img_copy = X_img.copy()\n",
    "    row, col, _ = X_img_copy.shape\n",
    "    salt_vs_pepper = 0.2\n",
    "    amount = 0.004\n",
    "    num_salt = np.ceil(amount * X_img_copy.size * salt_vs_pepper)\n",
    "    num_pepper = np.ceil(amount * X_img_copy.size * (1.0 - salt_vs_pepper))\n",
    "    \n",
    "        # Add Salt noise\n",
    "    coords0 = np.random.randint(0, row-1, int(num_salt)) \n",
    "    coords1 = np.random.randint(0, col-1, int(num_salt)) \n",
    "\n",
    "    X_img[coords0, coords1, :] = 255\n",
    "\n",
    "        # Add Pepper noise\n",
    "    coords0 = np.random.randint(0, row-1, int(num_pepper)) \n",
    "    coords1 = np.random.randint(0, col-1, int(num_pepper)) \n",
    "    \n",
    "    X_img[coords0, coords1, :] = 0\n",
    "    return X_img\n",
    "\n",
    "def transform_image(img,ang_range=16,shear_range=8,trans_range=3):\n",
    "    '''\n",
    "    This function transforms images to generate new images.\n",
    "    The function takes in following arguments,\n",
    "    1- Image\n",
    "    2- ang_range: Range of angles for rotation\n",
    "    3- shear_range: Range of values to apply affine transform to\n",
    "    4- trans_range: Range of values to apply translations over. \n",
    "    \n",
    "    A Random uniform distribution is used to generate different parameters for transformation\n",
    "    \n",
    "    '''\n",
    "    # Rotation\n",
    "\n",
    "    ang_rot = np.random.uniform(ang_range)-ang_range/2\n",
    "    rows,cols,ch = img.shape    \n",
    "    Rot_M = cv2.getRotationMatrix2D((cols/2,rows/2),ang_rot,1)\n",
    "\n",
    "    # Translation\n",
    "    tr_x = trans_range*np.random.uniform()-trans_range/2\n",
    "    tr_y = trans_range*np.random.uniform()-trans_range/2\n",
    "    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])\n",
    "\n",
    "    # Shear\n",
    "    pts1 = np.float32([[5,5],[20,5],[5,20]])\n",
    "\n",
    "    pt1 = 5+shear_range*np.random.uniform()-shear_range/2\n",
    "    pt2 = 20+shear_range*np.random.uniform()-shear_range/2\n",
    "    \n",
    "    # Brightness \n",
    "    \n",
    "\n",
    "    pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])\n",
    "\n",
    "    shear_M = cv2.getAffineTransform(pts1,pts2)\n",
    "        \n",
    "    img = cv2.warpAffine(img,Rot_M,(cols,rows))\n",
    "    img = cv2.warpAffine(img,Trans_M,(cols,rows))\n",
    "    #img = cv2.warpAffine(img,shear_M,(cols,rows))\n",
    "    \n",
    "    img = brightness_augment(img)\n",
    "    img = add_salt_pepper_noise(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(train_data,train_labels,augmentation_required_dict):\n",
    "    for key,val in augmentation_required_dict.items():\n",
    "        #print(key,val)\n",
    "        possible_indices=np.nonzero(train_labels==key)[0]\n",
    "        \n",
    "        for i in range(val):\n",
    "            index=np.random.choice(possible_indices)\n",
    "            new_image=transform_image(train_data[index])\n",
    "            new_label=train_labels[index]\n",
    "            train_data=np.concatenate((train_data,new_image.reshape(1,32,32,3)))\n",
    "            train_labels=np.append(train_labels,new_label)\n",
    "        print('finishing the augmentation for the key: ',key)\n",
    "    return train_data,train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_augmented=False\n",
    "\n",
    "if not ( is_augmented):\n",
    "    X_train,y_train=augment_data(images,labels,augmentation_required_train)\n",
    "    is_augmented=True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train={'data':X_train,'label':y_train}\n",
    "file_destination =  'data/train_augmented.p'\n",
    "import pickle\n",
    "with open(file_destination, 'wb') as handle:\n",
    "    pickle.dump(new_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "\"\"\"\n",
    "with open('file_destination', 'rb') as handle:\n",
    "    X_train = pickle.load(handle)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_destination =  'data/train_augmented.p'\n",
    "\n",
    "with open(file_destination, 'rb') as handle:\n",
    "    new_train = pickle.load(handle)\n",
    "X_train=new_train['data']\n",
    "y_train=new_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(images,dtype=np.float32)\n",
    "type(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "is_normalized=False\n",
    "\n",
    "def normalize(Dataset):\n",
    "    normalized = (np.array(Dataset,dtype=np.float32)-128)/128.0\n",
    "    return normalized   \n",
    "if not(is_normalized):\n",
    "    is_normalized=True\n",
    "    X_train = normalize (X_train)\n",
    "    #X_valid = normalize (X_valid)\n",
    "    X_test  = normalize (test_images)\n",
    "print(np.min(X_train))\n",
    "print(X_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying an old architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define your architecture here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "from tensorflow.contrib.layers import flatten\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "n_classes=43\n",
    "def LeNet(x,n_classes,keep_prob_arg):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    input_channels=3\n",
    "    filter_size=5\n",
    "    out_channels1=12\n",
    "    out_channels2=32\n",
    "    out_channels3=200\n",
    "    weights = {\n",
    "        # The shape of the filter weight is (height, width, input_depth, output_depth)\n",
    "        'conv1': tf.Variable(tf.truncated_normal(shape=(filter_size, filter_size, input_channels, out_channels1), mean = mu, stddev = sigma)),\n",
    "        'conv2': tf.Variable(tf.truncated_normal(shape=(filter_size, filter_size, out_channels1, out_channels2), mean = mu, stddev = sigma)),\n",
    "        'fl1': tf.Variable(tf.truncated_normal(shape=(5 * 5 * out_channels2, out_channels3), mean = mu, stddev = sigma)),\n",
    "        'fl2': tf.Variable(tf.truncated_normal(shape=(out_channels3, 84), mean = mu, stddev = sigma)),\n",
    "        'out': tf.Variable(tf.truncated_normal(shape=(84, n_classes), mean = mu, stddev = sigma))\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        # The shape of the filter bias is (output_depth,)\n",
    "        'conv1': tf.Variable(tf.zeros(out_channels1)),\n",
    "        'conv2': tf.Variable(tf.zeros(out_channels2)),\n",
    "        'fl1': tf.Variable(tf.zeros(out_channels3)),\n",
    "        'fl2': tf.Variable(tf.zeros(84)),\n",
    "        'out': tf.Variable(tf.zeros(n_classes))\n",
    "    }\n",
    "    # TODO: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28xout_channels1.\n",
    "    conv1=tf.nn.conv2d(\n",
    "    x,\n",
    "    weights['conv1'],\n",
    "    [1,1,1,1],\n",
    "    'VALID'\n",
    "    )\n",
    "    conv1 = tf.nn.bias_add(conv1, biases['conv1'])\n",
    "    # Activation.\n",
    "    #conv1= tf.nn.batch_normalization(conv1,mu,sigma)\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # TODO: Pooling. Input = 28x28x12. Output = 14x14x12.\n",
    "    ksize_pooling = [1, 2, 2, 1]\n",
    "    strides_pooling = [1, 2, 2, 1]\n",
    "    pool1=tf.nn.max_pool(conv1, ksize_pooling, strides_pooling, 'VALID')\n",
    "\n",
    "    \n",
    "    # TODO: Layer 2: Convolutional. Output = 10x10x32.\n",
    "    conv2=tf.nn.conv2d(\n",
    "    pool1,\n",
    "    weights['conv2'],\n",
    "    [1,1,1,1],\n",
    "    'VALID'\n",
    "    )\n",
    "    conv2 = tf.nn.bias_add(conv2, biases['conv2'])\n",
    "    # Activation.\n",
    " #   conv2= tf.nn.batch_normalization(conv2,mu,sigma)\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # TODO: Pooling. Input = 10x10x32. Output = 5x5x32\n",
    "    ksize_pooling = [1, 2, 2, 1]\n",
    "    strides_pooling = [1, 2, 2, 1]\n",
    "    pool2=tf.nn.max_pool(conv2, ksize_pooling, strides_pooling, 'VALID')\n",
    "\n",
    "\n",
    "\n",
    "    # TODO: Flatten. Input = 5x5x32. Output = 800.\n",
    "    flatt_pool2=flatten(pool2)\n",
    "    \n",
    "    # TODO: Layer 3: Fully Connected. Input = 800. Output = 200.\n",
    "    \n",
    "    # Layer 3: Fully Connected. Input = 800. Output = 200.\n",
    "    fl1 = tf.add(tf.matmul(flatt_pool2, weights['fl1']), biases['fl1'])\n",
    "    # Activation.\n",
    "    fl1 = tf.nn.relu(fl1)\n",
    "    fl1 = tf.nn.dropout(fl1, keep_prob_arg)\n",
    "\n",
    "    # TODO: Layer 4: Fully Connected. Input = 200. Output = 84.\n",
    "    \n",
    "    fl2 = tf.add(tf.matmul(fl1, weights['fl2']), biases['fl2'])\n",
    "    # Activation.\n",
    "    fl2 = tf.nn.relu(fl2)\n",
    "    fl2 = tf.nn.dropout(fl2, keep_prob_arg)\n",
    "    # TODO: Layer 5: Fully Connected. Input = 84. Output = 43.\n",
    "    logits = tf.add(tf.matmul(fl2, weights['out']), biases['out'])\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-1f20eac4a3d1>:14: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "keep_prob= tf.placeholder(tf.float32)\n",
    "\n",
    "one_hot_y = tf.one_hot(y, n_classes)\n",
    "\n",
    "\n",
    "rate = 0.0008\n",
    "EPOCHS = 7\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "logits = LeNet(x,n_classes,keep_prob)\n",
    "max_logits = tf.argmax(logits, 1)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y,keep_prob:1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Training Accuracy = 0.724\n",
      "\n",
      "EPOCH 2 ...\n",
      "Training Accuracy = 0.893\n",
      "\n",
      "EPOCH 3 ...\n",
      "Training Accuracy = 0.938\n",
      "\n",
      "EPOCH 4 ...\n",
      "Training Accuracy = 0.956\n",
      "\n",
      "EPOCH 5 ...\n",
      "Training Accuracy = 0.964\n",
      "\n",
      "EPOCH 6 ...\n",
      "Training Accuracy = 0.977\n",
      "\n",
      "EPOCH 7 ...\n",
      "Training Accuracy = 0.980\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "### Train your model here.\n",
    "### Calculate and report the accuracy on the training and validation set.\n",
    "### Once a final model architecture is selected, \n",
    "### the accuracy on the test set should be calculated and reported as well.\n",
    "### Feel free to use as many code cells as needed.\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob : 0.6})\n",
    "            \n",
    "        #validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        training_accuracy = evaluate(X_train, y_train)\n",
    "\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        #print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        #print()\n",
    "        print(\"Training Accuracy = {:.3f}\".format(training_accuracy))\n",
    "        print()\n",
    "    saver.save(sess, './first_LENET')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .\\first_LENET\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    #test_accuracy = evaluate(X_test, y_test)\n",
    "    #print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_input):\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    logits_evaluated= sess.run(max_logits,feed_dict={x:X_input,keep_prob:1.0})\n",
    "    return logits_evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .\\first_LENET\n",
      "[10 18  1 ... 23 10 33]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    test_predict = predict(X_test)\n",
    "    print(test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 18  1 ... 23 10 33]\n"
     ]
    }
   ],
   "source": [
    "print(test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csvData=[]\n",
    "csvData.append(['file_id','label'])\n",
    "cont=0\n",
    "for i in os.listdir('test_files'):\n",
    "    csvData.append([i,test_predict[cont]])\n",
    "    cont+=1\n",
    "\n",
    "\n",
    "with open('submission.csv', 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows(csvData)\n",
    "\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11770\n"
     ]
    }
   ],
   "source": [
    "print(len(test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
